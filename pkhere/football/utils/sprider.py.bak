#-*- coding: UTF-8 -*-
import urllib2
import urllib
import time
import cookielib
import re
from bs4 import BeautifulSoup

MATCH_CNT = 0
MATCH_INFO = {}
OLD_MATCH_INFO ={}
PREFIX_PATH = "https://mobile.28365365.com/sport"
class Sprider(object):
    _instance = None
    def __init__(self):
        self.headers  = {
                        'Accept':'application/x-ms-application, image/jpeg, application/xaml+xml, image/gif, image/pjpeg, application/x-ms-xbap, */*',
                        #'Accept-Encoding':'gzip, deflate',
                        'Accept-Language':'zh-CN,zh;q=0.8',
                        'Connection': 'Keep-Alive',
                        'Host': 'mobile.28365365.com',
                        'Referer': 'https://mobile.28365365.com/sport/splash/default.aspx?key=1',
                        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36 QQBrowser/2.0.1128.400',
                        'Cookie':'LastInteractionDate=1393605748716$0; usdi=uqid=5D443310-36D1-45A8-AD21-7306A89FA94B; session=processform=0; aps03=lng=10&tzi=27&ct=42&cst=132&cg=0&oty=1; pstk=A08E2207D93D4FE396D1CA78B5A4592D000003; lng=10',

                        }
        self.postData = {'Sport': 1,
                         'key': 1,
                         'L': 2,
                         'ip': 1,
                         'accesskey':'1',
                        }
        return


    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(Sprider, cls).__new__(cls, *args, **kwargs)
        return cls._instance
    #通过url获取网页的内容
    def getContentByUrl(self,urlPath):
        if urlPath == '':
            return ''


        cj = cookielib.LWPCookieJar()
        cookie_support = urllib2.HTTPCookieProcessor(cj)
        opener = urllib2.build_opener(cookie_support, urllib2.HTTPHandler)
        urllib2.install_opener(opener)

        #postdata = urllib.urlencode(self.postData)
        request = urllib2.Request(urlPath,headers=self.headers)
        #print request.get_full_url(),request.get_data(),postdata,dir(request)
        content = urllib2.urlopen(request).read()
        print 'getContentByUrl:',content
        return content
    def parseContentByUrl(self,urlPath):
        content = self.getContentByUrl(urlPath)
        self.parseMatchInfoByContent(content)


    def parseMatchInfoByContent(self,content):
        global MATCH_CNT,MATCH_INFO,OLD_MATCH_INFO
        soup = BeautifulSoup(content)
        s = soup.findAll('a',attrs={'href':re.compile('../coupon/'),} )
        #matchNumber = self.getMatchNumbers(s)
        #print s
        i = 0
        for j in s:
            if j.text == '' or j.text.find('v') == -1 or j.text.find(':')==-1 or j.text.find('-')==-1:
                continue
            linkpath = PREFIX_PATH+j['href'][2:]
            index = self.getIndexInOld(linkpath)
            if index >=0:
                team1 = OLD_MATCH_INFO[index]['team1']
                team2 = OLD_MATCH_INFO[index]['team2']
            else:
                MATCH_INFO[i]={}
                team1,team2 = self.getTeamSplitName(j)
                MATCH_INFO[i]['linkpath'] = PREFIX_PATH+j['href'][2:]
                MATCH_INFO[i]['daxiaoqiu'] = u'停盘'
                MATCH_INFO[i]['rangfen'] = u'停盘'
            score1,score2 = self.getTeamSplitScore(j)
            time1,time2 = self.getMatchTime(j)
            MATCH_INFO[i]['team1'] = team1
            MATCH_INFO[i]['team2'] = team2
            MATCH_INFO[i]['score1'] = score1
            MATCH_INFO[i]['score2'] = score2
            MATCH_INFO[i]['time1'] = time1
            MATCH_INFO[i]['time2'] = time2
            i+=1
        matchNumber = i
        for i in MATCH_INFO:
            print str(MATCH_INFO[i]['time1'])+":"+str(MATCH_INFO[i]['time1']) +"     " + MATCH_INFO[i]['team1'] +"vs"+MATCH_INFO[i]['team2'] +"  " + MATCH_INFO[i]['linkpath']
        OLD_MATCH_INFO = MATCH_INFO

    def getIndexInOld(self,path):
        global OLD_MATCH_INFO
        for i in OLD_MATCH_INFO:
            if OLD_MATCH_INFO[i]['linkpath'] == path:
                return i
        return -1

    def getTeamSplitName(self,contents):
        try:
            teamNames = contents.contents[0]
        except:
            teamNames = 'NA v NA'
        names = teamNames.split('v')
        return names[0],names[1]

    def getTeamSplitScore(self,contents):
        try:
            teamScores = contents.contents[2].text
            if not teamScores:
                teamScores = '0-0'
        except:
            teamScores = '0-0'
        scores = teamScores.split('-')
        return scores[0],scores[1]

    def getMatchTime(self,contents):
        try:
            matchTimes = contents.contents[3].text
            if not matchTimes:
                matchTimes = '00:00'
        except:
            matchTimes = '00:00'
        times = matchTimes.split(':')
        return times[0],times[1]

    def getMatchDetails(self,urlPath):
        print time.time()
        content = urllib2.urlopen(urlPath).read()
        soup = BeautifulSoup(content)
        s = soup.find('div',id='ctl00_Main_ctl00_ctl02_ctl01_Lines')
        #print s.text
        s = soup.find('div',id='ctl00_Main_ctl00_ctl06_ctl01_Lines')
        print time.time()
        #print s.text

sp = Sprider()
#urlPath = 'https://mobile.28365365.com/sport/splash/Default.aspx?Sport=1&key=1&L=2&ip=
urlPath = 'http://www.28365365.com/lite/#!op=14;cid=9998;cpid=15263385052C1_10_0'
#sp.parseContentByUrl(urlPath)
#sp.getMatchDetails('https://mobile.28365365.com/sport/coupon/?ptid=0&key=1-1-5-26336398-2-0-0-1-1-0-0-0-0-0-1-0-0-0-0-0-0')
sp.getContentByUrl(urlPath)